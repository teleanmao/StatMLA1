{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Pre-processing\n",
    "#Create the complete graph using networkx\n",
    "#And read the training data.\n",
    "\n",
    "import networkx as nx\n",
    "G=nx.Graph() #Graph\n",
    "trainingset={} #training data \n",
    "\n",
    "sinklist=set() #set of all unqiue sinks  \n",
    "\n",
    "with open('train.txt', 'r') as train:\n",
    "    \n",
    "    for line in train:\n",
    "        \n",
    "        edgelist= line.split()\n",
    "        \n",
    "        source=int(edgelist[0])\n",
    "        sinklength= len(edgelist)-1\n",
    "        sinks=[]\n",
    "    \n",
    "        for i in range(sinklength):\n",
    "            \n",
    "            G.add_edge(source, int(edgelist[i+1]))\n",
    "            \n",
    "            sinks.append(int(edgelist[i+1])) \n",
    "            sinklist.add(int(edgelist[i+1])) #update sinks set\n",
    "        trainingset[source]=sinks #update training set\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomly create edges for True samples\n",
    "\n",
    "import random \n",
    "truesamples=[]\n",
    "samplesize= 60000\n",
    "\n",
    "for i in range(samplesize):\n",
    "    \n",
    "    source, sinks = random.choice(list(trainingset.items()))\n",
    "    sink = random.choice(list(sinks))\n",
    "    truesamples.append((source,sink))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomly create edges for False samples\n",
    "\n",
    "falsesamples=[]\n",
    "samplesize= 60000\n",
    "\n",
    "for i in range(samplesize):\n",
    "    \n",
    "    source, sinks = random.choice(list(trainingset.items()))\n",
    "    sink = random.choice(list(sinklist))\n",
    "    if sink not in sinks:\n",
    "        falsesamples.append((source,sink))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Extraction \n",
    "\n",
    "#For true samples:\n",
    "truesampleextraction=[]\n",
    "for sample in truesamples:\n",
    "    feature=[]\n",
    "    iterator= nx.resource_allocation_index(G,[sample])\n",
    "    for u, v, p in iterator:\n",
    "        feature.append(p)\n",
    "    iterator= nx.jaccard_coefficient(G,[sample])\n",
    "    for u, v, p in iterator:\n",
    "        feature.append(p)\n",
    "    iterator= nx.adamic_adar_index(G,[sample])\n",
    "    for u, v, p in iterator:\n",
    "        feature.append(p)\n",
    "    iterator= nx.preferential_attachment(G,[sample])\n",
    "    for u, v, p in iterator:\n",
    "        feature.append(p)\n",
    "    iterator= nx.common_neighbor_centrality(G,[sample])\n",
    "    for u, v, p in iterator:\n",
    "        feature.append(p)\n",
    "    feature.append(1)\n",
    "    truesampleextraction.append(feature)\n",
    "    \n",
    "#For false samples:\n",
    "falsesampleextraction=[]\n",
    "for sample in truesamples:\n",
    "    feature=[]\n",
    "    iterator= nx.resource_allocation_index(G,[sample])\n",
    "    for u, v, p in iterator:\n",
    "        feature.append(p)\n",
    "    iterator= nx.jaccard_coefficient(G,[sample])\n",
    "    for u, v, p in iterator:\n",
    "        feature.append(p)\n",
    "    iterator= nx.adamic_adar_index(G,[sample])\n",
    "    for u, v, p in iterator:\n",
    "        feature.append(p)\n",
    "    iterator= nx.preferential_attachment(G,[sample])\n",
    "    for u, v, p in iterator:\n",
    "        feature.append(p)\n",
    "    iterator= nx.common_neighbor_centrality(G,[sample])\n",
    "    for u, v, p in iterator:\n",
    "        feature.append(p)\n",
    "    feature.append(0)\n",
    "    falsesampleextraction.append(feature)\n",
    "\n",
    "#For testing data:\n",
    "testingdata=[]\n",
    "with open('test-public.txt','r') as test:\n",
    "    next(f)\n",
    "    for line in test:\n",
    "        splitted = line.split()\n",
    "        testingdata.append((int(splitted[1]), int(splitted[2])))\n",
    "\n",
    "            \n",
    "    \n",
    "testingextraction=[]\n",
    "for sample in testingdata:\n",
    "    feature=[]\n",
    "    iterator= nx.resource_allocation_index(G,[sample])\n",
    "    for u, v, p in iterator:\n",
    "        feature.append(p)\n",
    "    iterator= nx.jaccard_coefficient(G,[sample])\n",
    "    for u, v, p in iterator:\n",
    "        feature.append(p)\n",
    "    iterator= nx.adamic_adar_index(G,[sample])\n",
    "    for u, v, p in iterator:\n",
    "        feature.append(p)\n",
    "    iterator= nx.preferential_attachment(G,[sample])\n",
    "    for u, v, p in iterator:\n",
    "        feature.append(p)\n",
    "    iterator= nx.common_neighbor_centrality(G,[sample])\n",
    "    for u, v, p in iterator:\n",
    "        feature.append(p)\n",
    "    \n",
    "    testingextraction.append(feature)\n",
    "\n",
    "#Create new training and testing sets:\n",
    "\n",
    "trainingsets=truesampleextraction + falsesampleextraction\n",
    "random.shuffle(trainingsets)\n",
    "\n",
    "import csv\n",
    "with open(\"train.csv\",\"w\",newline=\"\") as file:\n",
    "    writer=csv.writer(file)\n",
    "    writer.writerows(trainingsets)\n",
    "\n",
    "\n",
    "with open(\"test.csv\",\"w\",newline=\"\") as file:\n",
    "    writer=csv.writer(file)\n",
    "    writer.writerows(testingextraction)    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
